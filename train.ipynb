{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-27 21:52:23.061786: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-27 21:52:23.061819: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import math\n",
    "import datetime\n",
    "from os import path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pydot\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.backend import dtype\n",
    "from tensorflow.python.keras.layers.core import Lambda\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "class Dataloader:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_width,\n",
    "        label_width,\n",
    "        shift=1,\n",
    "        train_ratio=0.6,\n",
    "        val_ratio=0.2,\n",
    "        test_ratio=0.2,\n",
    "        batch_size=256,\n",
    "    ):\n",
    "        self.input_width = input_width\n",
    "        self.label_width = label_width\n",
    "        self.shift = shift\n",
    "        self.total_window_size = self.input_width + self.shift\n",
    "        self.input_slice = slice(0, self.input_width)\n",
    "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
    "        self.label_start = self.total_window_size - self.label_width\n",
    "        self.labels_slice = slice(self.label_start, None)\n",
    "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
    "        self.batch_size = batch_size\n",
    "        self.scale_coef = 1000.0\n",
    "        self.bias = 0.0\n",
    "        self.clip_value = 4.0\n",
    "\n",
    "    def load_tsv(\n",
    "        self,\n",
    "        tsv_filename,\n",
    "        input_column=\"open\",\n",
    "        train_ratio=0.6,\n",
    "        val_ratio=0.2,\n",
    "        test_ratio=0.2,\n",
    "        verbose=1,\n",
    "    ):\n",
    "        df = pd.read_csv(\n",
    "            tsv_filename,\n",
    "            sep=\"\\t\",\n",
    "            header=0,\n",
    "            dtype={\n",
    "                \"open\": np.float32,\n",
    "                \"close\": np.float32,\n",
    "                \"tickvol\": np.float32,\n",
    "                \"vol\": np.float32,\n",
    "            },\n",
    "            names=[\"date\",\"time\",\"open\",\"high\",\"low\",\"close\",\"tickvol\",\"vol\",\"spread\",],\n",
    "        )\n",
    "        df_size = df[input_column].size\n",
    "        train_size = int(df_size * train_ratio)\n",
    "        val_size = int(df_size * val_ratio)\n",
    "        test_size = int(df_size * test_ratio)\n",
    "        train_slice = slice(-train_size, None)\n",
    "        val_slice = slice(-(train_size + val_size), -train_size)\n",
    "        test_slice = slice(\n",
    "            -(train_size + val_size + test_size), -(train_size + val_size)\n",
    "        )\n",
    "        self.train_df = df[input_column][train_slice]\n",
    "        self.val_df = df[input_column][val_slice]\n",
    "        self.test_df = df[input_column][test_slice]\n",
    "        if verbose == 1:\n",
    "            print(self.__sizes__())\n",
    "            print(self.__repr__())\n",
    "        return True\n",
    "\n",
    "    def load_df(\n",
    "        self,\n",
    "        df: pd.core.frame.DataFrame,\n",
    "        input_column=\"open\",\n",
    "        train_ratio=0.6,\n",
    "        val_ratio=0.2,\n",
    "        test_ratio=0.2,\n",
    "        verbose=1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Преобразует dataframe в обучающую выборку\n",
    "        dataframe - Pandas Dataframe с колонками [\"date\", \"time\", \"open\", \"high\", \"low\", \"close\", \"tickvol\", \"spread\", \"real_volume\"]\n",
    "        input_column - колонка с основными данными\n",
    "        \"\"\"\n",
    "        df_size = df[input_column].size\n",
    "        train_size = int(df_size * train_ratio)\n",
    "        val_size = int(df_size * val_ratio)\n",
    "        test_size = int(df_size * test_ratio)\n",
    "        train_slice = slice(-train_size, None)\n",
    "        val_slice = slice(-(train_size + val_size), -train_size)\n",
    "        test_slice = slice(\n",
    "            -(train_size + val_size + test_size), -(train_size + val_size)\n",
    "        )\n",
    "        self.train_df = df[input_column][train_slice]\n",
    "        self.val_df = df[input_column][val_slice]\n",
    "        self.test_df = df[input_column][test_slice]\n",
    "        if verbose == 1:\n",
    "            print(self.__sizes__())\n",
    "            print(self.__repr__())\n",
    "        return True\n",
    "\n",
    "    def __sizes__(self):\n",
    "        return \"\\n\".join(\n",
    "            [\n",
    "                f\"Размер train: {len(self.train_df)}\",\n",
    "                f\"Размер validation: {len(self.val_df)}\",\n",
    "                f\"Размер test: {len(self.test_df)}\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"\\n\".join(\n",
    "            [\n",
    "                f\"Размер окна: {self.total_window_size}\",\n",
    "                f\"Размер входа: {self.input_width}\",\n",
    "                f\"Размер выхода: {self.label_width}\",\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def make_dataset(self, data):\n",
    "        ds = self.transform(data)\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "            data=ds,\n",
    "            targets=None,\n",
    "            sequence_length=self.total_window_size,\n",
    "            sequence_stride=1,\n",
    "            shuffle=True,\n",
    "            batch_size=self.batch_size,\n",
    "        )\n",
    "        ds = ds.map(self.split_window)\n",
    "        return ds\n",
    "\n",
    "    def make_input(self, data):\n",
    "        ds = self.transform(data)\n",
    "        ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "            data=ds,\n",
    "            targets=None,\n",
    "            sequence_length=self.input_width,\n",
    "            sequence_stride=1,\n",
    "            shuffle=False,\n",
    "        )\n",
    "        return ds\n",
    "\n",
    "    def make_output(self, output_data):\n",
    "        d = self.inverse_transform(output_data)\n",
    "        return d\n",
    "\n",
    "    def split_window(self, databatch):\n",
    "        inputs = databatch[:, self.input_slice]\n",
    "        labels = databatch[:, self.labels_slice]\n",
    "        inputs.set_shape([None, self.input_width])\n",
    "        labels.set_shape([None, self.label_width])\n",
    "        return inputs, labels\n",
    "\n",
    "    def transform(self, data):\n",
    "        # ds = tf.math.subtract(data, data[:, 0:1])\n",
    "        # self.first_value = data[0:1]\n",
    "        ds = np.diff(data) * self.scale_coef + self.bias\n",
    "        std = np.std(ds)\n",
    "        ds = np.clip(ds, -std * self.clip_value, std * self.clip_value)\n",
    "        # ds = np.concatenate((ds[0:1], ds))\n",
    "        # ds = np.diff(ds) * self.scale_coef + self.bias\n",
    "        # ds = np.log(ds)\n",
    "        return ds\n",
    "\n",
    "    def inverse_transform(self, output_data):\n",
    "        d = (output_data - self.bias) / self.scale_coef\n",
    "        return d\n",
    "\n",
    "    def moving_average(self, a, n=1):\n",
    "        if n == 0:\n",
    "            return a\n",
    "        result = np.convolve(a, np.ones((n,)) / float(n), mode=\"valid\")\n",
    "        return result\n",
    "\n",
    "    def shift_to_zero(self, data):\n",
    "        data = tf.math.subtract(data, data[:, 0:1])  # сдвиг начального значения в ноль\n",
    "        # data, data[:, self.input_width - 1 : self.input_width, :]\n",
    "        return data\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        return self.make_dataset(self.train_df)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self.make_dataset(self.val_df)\n",
    "\n",
    "    @property\n",
    "    def test(self):\n",
    "        return self.make_dataset(self.test_df)\n",
    "\n",
    "class ClippedMSE(losses.Loss):\n",
    "    def __init__(\n",
    "        self,\n",
    "        value_min=-1.0,\n",
    "        value_max=1.0,\n",
    "        reduction=losses.Reduction.AUTO,\n",
    "        name=\"clipped_mse\",\n",
    "    ) -> None:\n",
    "        super().__init__(reduction=reduction, name=name)\n",
    "        self.value_min = value_min\n",
    "        self.value_max = value_max\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        clipped_y_pred = tf.clip_by_value(y_pred, self.value_min, self.value_max)\n",
    "        clipped_y_true = tf.clip_by_value(y_true, self.value_min, self.value_max)\n",
    "        return losses.mean_squared_error(clipped_y_true, clipped_y_pred)\n",
    "\n",
    "\n",
    "class ClippedMAE(losses.Loss):\n",
    "    def __init__(self, value_min=-1.0, value_max=1.0, reduction=losses.Reduction.AUTO, name=\"clipped_mae\") -> None:\n",
    "        super().__init__(reduction=reduction, name=name)\n",
    "        self.value_min = value_min\n",
    "        self.value_max = value_max\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        clipped_y_pred = tf.clip_by_value(y_pred, self.value_min, self.value_max)\n",
    "        clipped_y_true = tf.clip_by_value(y_true, self.value_min, self.value_max)\n",
    "        return losses.mean_absolute_error(clipped_y_true, clipped_y_pred)\n",
    "\n",
    "def ConvAdaptiveKernelSize(x, activation, filters=8, kernel_size=2, dropout=0.5):\n",
    "    k_size = kernel_size if x.shape[-2] >= kernel_size else x.shape[-2]\n",
    "    l2 = keras.regularizers.l2(1e-10)\n",
    "    x = Conv1D(filters, k_size, padding=\"valid\")(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    x = Lambda(activation)(x)\n",
    "    # x = Dropout(rate=dropout)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def red(\n",
    "    input_width,\n",
    "    out_width,\n",
    "    columns=16,\n",
    "    lr=1e-2,\n",
    "    min_v=-2,\n",
    "    max_v=2,\n",
    "    training=True,\n",
    "    name=\"red\",\n",
    "):\n",
    "    if training:\n",
    "        dropout = 1.0 / 256.0\n",
    "    else:\n",
    "        dropout = 0\n",
    "\n",
    "    init = keras.initializers.RandomUniform(-1024, 1024)\n",
    "    l2 = keras.regularizers.L2(l2=1e-10)\n",
    "    dct_length = input_width\n",
    "    def f_mean(z): return tf.math.reduce_mean(z, 1, keepdims=True)\n",
    "    def f_logtanh(x): return tf.math.log(tf.exp(1.0) + tf.abs(x)) * tf.tanh(x)\n",
    "    def f_dct(x): return tf.signal.dct(x, n=dct_length, norm='ortho')\n",
    "    # def f_dct(x): return tf.signal.mdct(\n",
    "    # x, frame_length = 8, norm = 'ortho', pad_end = True)\n",
    "    n = int(math.log2(input_width))\n",
    "    filters = 32\n",
    "    inputs = Input(shape=(input_width,))\n",
    "    # key = [Lambda(lambda z: z[:, -(2 ** (i+1)) :])(inputs) for i in range(n)]\n",
    "    # m = [Lambda(f_mean, name=f\"mean{i}\")(key[i]) for i in range(n)]\n",
    "    # m = Concatenate(name=f\"concat_means\")(m)\n",
    "    # m = Dense(filters)(m)\n",
    "    # m = Reshape((1, -1))(m)\n",
    "    f = Lambda(f_dct, name=f\"dct\")(inputs)\n",
    "    f = Reshape((-1, 1))(f)\n",
    "    while f.shape[-2] > 1:\n",
    "        f = ConvAdaptiveKernelSize(f, tf.nn.tanh, filters, 16, dropout)\n",
    "    # f = Dropout(rate=dropout)(f)\n",
    "    # x = Multiply()([m, f])\n",
    "    # x = Flatten()(f)\n",
    "    # x = Dense(64)(x)\n",
    "    x = Reshape((-1, 1))(f)\n",
    "    x = LSTM(64, return_sequences=True, dropout=dropout, name=\"lstm-1\")(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(32, name=f\"d-in-0\")(x)\n",
    "    rows_count = 8\n",
    "    units = 32\n",
    "    z = [Dense(units, name=f\"d-in{c}-{0}\")(x) for c in range(columns)]\n",
    "    z = [Lambda(f_logtanh)(z[c]) for c in range(columns)]\n",
    "    for c in range(columns):\n",
    "        for r in range(rows_count - 1):\n",
    "            z[c] = Dense(units, name=f\"d{c}-{r}\")(z[c])\n",
    "            z[c] = BatchNormalization()(z[c])g\n",
    "            z[c] = Lambda(f_logtanh)(z[c])\n",
    "        z[c] = Dense(out_width)(z[c])\n",
    "        # z[c] = Lambda(f_logtanh)(z[c])\n",
    "    x = Concatenate()(z)\n",
    "    x = Dense(out_width)(x)\n",
    "    # x = Lambda(f_logtanh)(x)\n",
    "    outputs = x\n",
    "    model = keras.Model(inputs, outputs, name=name)\n",
    "    MAE = keras.metrics.MeanAbsoluteError()\n",
    "    CMSE = ClippedMSE(min_v, max_v)\n",
    "    CMAE = ClippedMAE(min_v, max_v)\n",
    "    model.compile(\n",
    "        # loss=keras.losses.Huber(),\n",
    "        # loss=keras.losses.MeanSquaredError(),\n",
    "        loss=CMSE,\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "        metrics=[MAE],\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        datafile,\n",
    "        model,\n",
    "        input_width,\n",
    "        label_width,\n",
    "        shift=None,\n",
    "        train_ratio=0.8,\n",
    "        val_ratio=0.1,\n",
    "        test_ratio=0.1,\n",
    "        batch_size=256,\n",
    "    ):\n",
    "        if shift == None:\n",
    "            shift = label_width\n",
    "        self.dataloader = Dataloader(\n",
    "            input_width=input_width,\n",
    "            label_width=label_width,\n",
    "            shift=shift,\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        if type(datafile) == str:\n",
    "            self.dataloader.load_tsv(\n",
    "                datafile,\n",
    "                input_column=\"open\",\n",
    "                train_ratio=train_ratio,\n",
    "                val_ratio=val_ratio,\n",
    "                test_ratio=test_ratio,\n",
    "            )\n",
    "        elif type(datafile) == pd.core.frame.DataFrame:\n",
    "            self.dataloader.load_df(\n",
    "                datafile,\n",
    "                input_column=\"open\",\n",
    "                train_ratio=train_ratio,\n",
    "                val_ratio=val_ratio,\n",
    "                test_ratio=test_ratio,\n",
    "            )\n",
    "        if type(model) == str:\n",
    "            self.load_model(model)\n",
    "        elif isinstance(model, tf.keras.Model):\n",
    "            self.model = model\n",
    "        else:\n",
    "            print(\n",
    "                \"Ошибка загрузки модели модели. Параметр model должен быть либо строкой, либо моделью keras\"\n",
    "            )\n",
    "\n",
    "    def __call__(self, data):\n",
    "        # x = self.dataloader.make_input(data)\n",
    "        return self.model(data[-self.dataloader.input_width - 1 :])\n",
    "\n",
    "    def load_model(self, filename, lr=1e-5):\n",
    "        # self.model = keras.models.load_model(self.name, custom_objects={\"shifted_mse\": shifted_mse})\n",
    "        self.model = keras.models.load_model(filename, compile=False)\n",
    "        self.model.compile(\n",
    "            loss=\"mse\",\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "            metrics=[keras.metrics.MeanAbsoluteError()],\n",
    "        )\n",
    "        return self.model\n",
    "\n",
    "    def save_model(self):\n",
    "        # self.model.save(\"models/\" + self.model.name + \".h5\")\n",
    "        self.model.save(\"models/\" + self.model.name)\n",
    "\n",
    "    def plot(self):\n",
    "        model_png_name = \"models/\" + self.model.name + \".png\"\n",
    "        plot_model(\n",
    "            self.model,\n",
    "            show_shapes=False,\n",
    "            show_layer_names=False,\n",
    "            rankdir=\"LR\",\n",
    "            to_file=model_png_name,\n",
    "        )\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        batch_size=256,\n",
    "        epochs=2,\n",
    "        use_tensorboard=True,\n",
    "        use_early_stop=True,\n",
    "        use_checkpoints=True,\n",
    "        use_multiprocessing=True,\n",
    "        verbose=1,\n",
    "    ):\n",
    "        callbacks = []\n",
    "        start_fit_time = datetime.datetime.now()\n",
    "        log_dir = \"logs/fit/\" + start_fit_time.strftime(\"%Y_%m_%d-%H_%M_%S\")\n",
    "        if use_checkpoints:\n",
    "            ckpt = \"ckpt/\" + self.model.name + \".ckpt\"\n",
    "            backup = keras.callbacks.ModelCheckpoint(\n",
    "                filepath=ckpt,\n",
    "                monitor=\"loss\",\n",
    "                save_weights_only=True,\n",
    "                save_best_only=True,\n",
    "            )\n",
    "            callbacks.append(backup)\n",
    "            try:\n",
    "                self.model.load_weights(ckpt)\n",
    "                if verbose > 0:\n",
    "                    print(\n",
    "                        \"Загружены веса последней контрольной точки \" + self.model.name\n",
    "                    )\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        if use_tensorboard:\n",
    "            tb = keras.callbacks.TensorBoard(log_dir=log_dir, write_graph=True)\n",
    "            callbacks.append(tb)\n",
    "        if use_early_stop:\n",
    "            es = keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                patience=2 ** 4,\n",
    "                min_delta=1e-5,\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "            callbacks.append(es)\n",
    "\n",
    "        history = self.model.fit(\n",
    "            self.dataloader.train,\n",
    "            validation_data=self.dataloader.val,\n",
    "            batch_size=batch_size,\n",
    "            epochs=epochs,\n",
    "            shuffle=True,\n",
    "            use_multiprocessing=use_multiprocessing,\n",
    "            verbose=verbose,\n",
    "            callbacks=callbacks,\n",
    "        )\n",
    "        end_fit_time = datetime.datetime.now()\n",
    "        delta_time = end_fit_time - start_fit_time\n",
    "        if verbose > 0:\n",
    "            print(f\"\\nВремя {start_fit_time}->{end_fit_time} : {delta_time}\")\n",
    "        return history\n",
    "\n",
    "    def evaluate(self):\n",
    "        return self.model.evaluate(self.dataloader.test, verbose=1)\n",
    "\n",
    "    def predict(self, data, verbose=0):\n",
    "        \"\"\"Вычисление результата для набора data - массив размерности n\"\"\"\n",
    "        x = self.dataloader.make_input(data)\n",
    "        f = self.model.predict(x, use_multiprocessing=True, verbose=verbose)\n",
    "        y = self.dataloader.make_output(f)\n",
    "        # result = self.dataloader.make_output(y)\n",
    "        return y\n",
    "\n",
    "    def iterate(self, inputs, steps=1):\n",
    "        results = []\n",
    "        # input_width + 1 нужно для вычисления np.diff\n",
    "        size = self.dataloader.input_width + 1\n",
    "        for i in range(0, steps):\n",
    "            inputs = inputs[-size:]\n",
    "            output = float(self.predict(inputs, verbose=0)[-1][0])\n",
    "            inputs = np.append(inputs, inputs[-1] + output)\n",
    "            results.append(output)\n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2 ** 10\n",
    "dataset_segment = 1.0 / 8.0\n",
    "input_width = 2 ** 8\n",
    "label_width = 1\n",
    "columns = 64\n",
    "\n",
    "model = red(\n",
    "    input_width,\n",
    "    label_width,\n",
    "    columns_count=columns,\n",
    "    lr=1e-4,\n",
    "    min_v=-2.0,\n",
    "    max_v=2.0,\n",
    "    training=True,\n",
    "    name=f\"eurusd-{columns}-{input_width}-{label_width}\",\n",
    ")\n",
    "\n",
    "predictor = Predictor(\n",
    "    datafile=\"datas/EURUSD_H1.csv\",\n",
    "    model=model,\n",
    "    input_width=input_width,\n",
    "    label_width=label_width,\n",
    "    train_ratio=1.0 - 1.0 * dataset_segment,\n",
    "    val_ratio=dataset_segment,\n",
    "    test_ratio=0,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "# predictor.plot()\n",
    "# predictor.model.summary()\n",
    "\n",
    "i = 0\n",
    "while True:\n",
    "    i += 1\n",
    "    print(f\"\\nМодель {model.name} проход №{i}\\n\")\n",
    "    history = predictor.fit(\n",
    "        use_tensorboard=False,\n",
    "        use_early_stop=False,\n",
    "        batch_size=batch_size,\n",
    "        epochs=2 ** 16,\n",
    "    )\n",
    "    predictor.save_model()\n",
    "    # perfomance = predictor.evaluate()\n",
    "    print(\"Модель обновлена\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
